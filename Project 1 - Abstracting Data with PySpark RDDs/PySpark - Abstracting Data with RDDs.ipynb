{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstracting Data with RDDs\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "#### What are RDDs?\n",
    "\n",
    "RDDs are called __Resilient Distributed Datasets__, where these are a collection of immutable JVM objects that are distributed across an Apache Spark Cluster. It is also the most fundamental dataset type for Apache Spark, whereby actions that are on a Spark DataFrame will get translated into highly optimised execution of transformations and actions on RDDs. \n",
    "\n",
    "The data would be split up into chunks based on a key and subsequently dispersed to all the executor nodes. The advantages of RDDs are its high resilience and ability to be recovered quickly as the same data chunks are replicated across multiple executor nodes. It also allows functional calculations on all the dataset quickly using mulitple nodes. Further, RDDs keep a log of the execution steps that were applied to each chunk which also combat against data lost by execution error. \n",
    "\n",
    "__This notebook will then go through the basics of using PySpark__.\n",
    "\n",
    "## 1 PySpark Machine Configuration:\n",
    "\n",
    "Here it only uses two processing cores from the CPU, and it set up by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>17</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td></td></tr><tr><td>19</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td></td></tr><tr><td>22</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"executorCores\" : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>23</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Creating RDDs:\n",
    "\n",
    "There are two ways to do this:\n",
    "- 1) Use the \"parallelize()\" method, a collection of lists or array of some elements.\n",
    "- 2) Reference a file(s) that are located either locally or from an external source.\n",
    "\n",
    "Here, the (1) method creates aparallelised collection where it would allow Spark to distribute the data to all the executor nodes and operate on it in parallel. For example, an operations done in parallel can be the \"reduceByKey(add)\" method applied as \"myRDD.reduceByKey(add)\".\n",
    "\n",
    "The \".take()\" method returns the values of the RDD to the console. Please note that the more common approach in PySpark is to use \".collect()\" moethod. However, this may prove to be taxing on most systems if the dataset is huge. The best way to use the \".take()\" method as it is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example to Creating a RDDs:\n",
    "myRDD = sc.parallelize( [('Mike', 19), ('June', 18), \n",
    "                         ('Rachel', 16), ('Rob', 18),\n",
    "                        ('Scott', 17)] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mike', 19), ('June', 18), ('Rachel', 16), ('Rob', 18), ('Scott', 17)]"
     ]
    }
   ],
   "source": [
    "# Inspect: to view what is inside an RDD.\n",
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Reading Data From Files:\n",
    "\n",
    "Here, a CSV file will be read and the datasets can be found in the \"Dataset\" folder that is avaible with this notebook download.\n",
    "\n",
    "The Datasets are:\n",
    "- 1) airport-codes.txt\n",
    "- 2) departure_delays.csv\n",
    "\n",
    "Source:\n",
    "- https://openflights.org/data.html\n",
    "- https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays-on-time-data\n",
    "\n",
    "NOTE: Make sure your Current Working Directory is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the Path:\n",
    "path = '++++your working directory here++++/Datasets/'\n",
    "os.chdir(path + 'Datasets/')\n",
    "folder_pathway = os.getcwd()\n",
    "\n",
    "# print(folder_pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the dataset: airport-codes.txt\n",
    "myRDD = (\n",
    "    sc.textFile(folder_pathway + '/airport-codes.txt',\n",
    "                minPartitions=4,\n",
    "                use_unicode=True\n",
    "    ).map(lambda element: element.split(\"\\t\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is happening here?\n",
    "\n",
    "After defining the pathway to the \"Datasets\" folder, there are two additional parameters added which are:\n",
    "- \"minPartitions\" that defines the number of partitions that make up the RDD, often without specifying this, the Spark engine will determine the best number based on file size. The user can set this partition value based on performance reasons.\n",
    "- \"use_unicode\" ensures the processing is in Unicode.\n",
    "\n",
    "Finally, there is a \".map()\" function, where it would transform the data from a list of strings to a lists of lists. There are also a lambda function used for mapping the transformation. This uses PySparks's split function to split the string according to the delimiter which is a tab. (\"\\t\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['City', 'State', 'Country', 'IATA'], ['Abbotsford', 'BC', 'Canada', 'YXX'], ['Aberdeen', 'SD', 'USA', 'ABR'], ['Abilene', 'TX', 'USA', 'ABI'], ['Akron', 'OH', 'USA', 'CAK']]"
     ]
    }
   ],
   "source": [
    "# Inspect: \n",
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the RDD:\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "# Determine the number of partitions that support the RDD:\n",
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the number of partitions when creating an RDD:\n",
    "\n",
    "The important aspect of using/setting the number of partitions on the RDD is that the more partitions set, the higher the parallelism, which may improve performance of query.\n",
    "\n",
    "## 3.1 Using a Larger Dataset:\n",
    "\n",
    "Here, the dataset used will be the \"departure_delays.csv\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the dataset: departure_delays.csv\n",
    "myRDD = (\n",
    "    sc.textFile(folder_pathway + '/departure_delays.csv').map(lambda element: element.split(\"\\t\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391579"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the RDD: this took about 2 secs.\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "# Determine the number of partitions that support the RDD:\n",
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how increasing the number of partitions can help the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the dataset: airport-codes.txt\n",
    "myRDD = (\n",
    "    sc.textFile(folder_pathway + '/departure_delays.csv',\n",
    "                minPartitions=8\n",
    "    ).map(lambda element: element.split(\"\\t\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391579"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the RDD: slightly faster than 2 secs.\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8"
     ]
    }
   ],
   "source": [
    "# Determine the number of partitions that support the RDD:\n",
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 More on RDD Transformations:\n",
    "\n",
    "Commonly, there are two types of operations that can be used on the RDD and these are:\n",
    "- Transformations, this transforms one RDD into another (like one or more output RDDs), these are only executed when the action is called on a RDD.\n",
    "- Actions, unlike transformations will return the raw values.\n",
    "\n",
    "## 4.1 Save the datasets into RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the data to the RDD: airports\n",
    "airports = (\n",
    "    sc.textFile(folder_pathway + '/airport-codes.txt').map(lambda element: element.split(\"\\t\"))\n",
    ")\n",
    "\n",
    "# Save the data to the RDD: flights\n",
    "flights = (\n",
    "    sc.textFile(folder_pathway + '/departure_delays.csv').map(lambda element: element.split(\",\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['City', 'State', 'Country', 'IATA'], ['Abbotsford', 'BC', 'Canada', 'YXX'], ['Aberdeen', 'SD', 'USA', 'ABR'], ['Abilene', 'TX', 'USA', 'ABI'], ['Akron', 'OH', 'USA', 'CAK']]"
     ]
    }
   ],
   "source": [
    "# Inspect: \n",
    "airports.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['date', 'delay', 'distance', 'origin', 'destination'], ['01011245', '6', '602', 'ABE', 'ATL'], ['01020600', '-8', '369', 'ABE', 'DTW'], ['01021245', '-2', '602', 'ABE', 'ATL'], ['01020605', '-4', '602', 'ABE', 'ATL']]"
     ]
    }
   ],
   "source": [
    "# Inspect: \n",
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Transformation type: .map()\n",
    "\n",
    "This .map(func) function will perform a transformation on the current RDD and returns a new RDD by passing each of the elements through a function \"func\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('City', 'State'), ('Abbotsford', 'BC'), ('Aberdeen', 'SD'), ('Abilene', 'TX'), ('Akron', 'OH')]"
     ]
    }
   ],
   "source": [
    "# Extract the first two columns:\n",
    "airports.map(lambda c: (c[0], c[1])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Transformation type: .filter()\n",
    "\n",
    "This .filter(func) function will perform a transformation on the current RDD and returns a new RDD based on selecting the elements where the function \"func\" returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bellingham', 'WA'), ('Moses Lake', 'WA'), ('Pasco', 'WA'), ('Pullman', 'WA'), ('Seattle', 'WA')]"
     ]
    }
   ],
   "source": [
    "# Filter for the 2nd column == \"WA\"\n",
    "(\n",
    "    airports\n",
    "    .map(lambda c: (c[0], c[1]))\n",
    "    .filter(lambda c: c[1] == \"WA\")\n",
    "    .take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Transformation type: .flatMap()\n",
    "\n",
    "The .flatMap(func) function would be similar to map, however, the new RDD flattens out all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bellingham', 'WA', 'Moses Lake', 'WA', 'Pasco', 'WA', 'Pullman', 'WA', 'Seattle', 'WA']"
     ]
    }
   ],
   "source": [
    "# Filter for the 2nd column == \"WA\", selects the first 2 columns within the RDD and proceeds to flatten out the values:\n",
    "(\n",
    "    airports\n",
    "    .filter(lambda c: c[1] == \"WA\")\n",
    "    .map(lambda c: (c[0], c[1]))\n",
    "    .flatMap(lambda x: x)\n",
    "    .take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Transformation type: .distinct() \n",
    "\n",
    "The distinct() function returns a transformed new RDD that contains the distinct elements of the original RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country', 'Canada', 'USA']"
     ]
    }
   ],
   "source": [
    "# Return the distinct elements for the 3rd column that is the countries data.\n",
    "(\n",
    "    airports\n",
    "    .map(lambda c: c[2])\n",
    "    .distinct()\n",
    "    .take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Transformation type: .sample()\n",
    "\n",
    "The sample(WithReplacement, fraction, seed) function will sample a fraction of the data, either with or without replacement based on random seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABQ', 'AEX', 'AGS', 'ANC', 'ATL']"
     ]
    }
   ],
   "source": [
    "# Provide a sample based on 0.001% of the flights RDD data that is specific to the 4th column (original city of the flight) without replacement (False) using random seed of 101.\n",
    "(\n",
    "    flights\n",
    "    .map(lambda c: c[3])\n",
    "    .sample(False, 0.001, 123)\n",
    "    .take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Transformation type: .join()\n",
    "\n",
    "The join(RDD') function transforms and returns a RDD of (key, (val_left, val_right)) when calling the RDD (key, val_left) and RDD (key, val_right), where the Outer joins are supported by left outer join, right outer join and full outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ABE', ('01011245', 'PA')), ('ABE', ('01020600', 'PA')), ('ABE', ('01021245', 'PA')), ('ABE', ('01020605', 'PA')), ('ABE', ('01031245', 'PA'))]"
     ]
    }
   ],
   "source": [
    "# Get Flight Data -> ('JFK', 01010900)\n",
    "flt = flights.map(lambda c: (c[3], c[0]))\n",
    "\n",
    "# Get Airport Data -> ('JFK', 'NY')\n",
    "\n",
    "airp = airports.map(lambda c: (c[3], c[1]))\n",
    "\n",
    "# Perform the inner join method between the RDDs:\n",
    "flt.join(airp).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Transformation type: .repartition()\n",
    "\n",
    "The repartition(n) will repartition the RDD into 'n' partitions, this is done randomly by reshuffling and uniformly redistributing the data across the network. This should help to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "# Check the original RDD partitions:\n",
    "flights.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "# Repartition the RDD: to 2 partitions.\n",
    "flights2 = flights.repartition(2)\n",
    "\n",
    "# Check the RDD partitions:\n",
    "flights2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8"
     ]
    }
   ],
   "source": [
    "# Repartition the RDD: to 2 partitions.\n",
    "flights8 = flights.repartition(8)\n",
    "\n",
    "# Check the RDD partitions:\n",
    "flights8.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Transformation type: .zipWithIndex()\n",
    "\n",
    "The zipWithIndex() function will append (Zips) the RDD with the elemnet indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('City', 'IATA'), 0), (('Abbotsford', 'YXX'), 1), (('Aberdeen', 'ABR'), 2), (('Abilene', 'ABI'), 3), (('Akron', 'CAK'), 4)]"
     ]
    }
   ],
   "source": [
    "# Preview each row within the RDD and its Index:\n",
    "zip_air = airports.map(lambda c: (c[0], c[3]))\n",
    "\n",
    "# Inspect:\n",
    "zip_air.zipWithIndex().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the header from the data:\n",
    "\n",
    "New changes to Python 3: https://www.python.org/dev/peps/pep-3113/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Abbotsford', 'YXX'), ('Aberdeen', 'ABR'), ('Abilene', 'ABI'), ('Akron', 'CAK'), ('Alamosa', 'ALS')]"
     ]
    }
   ],
   "source": [
    "# Previously:\n",
    "# (\n",
    "#     zip_air\n",
    "#     .zipWithIndex()\n",
    "#     .filter(lambda (row, idx): idx > 0)\n",
    "#     .map(lambda (row, idx): row)\n",
    "#     .take(5)\n",
    "# )\n",
    "\n",
    "# NEW:\n",
    "# Using the zipWithIndex function to skip the header row. -> filter out row 0 (column HEADER) and extract only row info.\n",
    "(\n",
    "    zip_air\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row_idx: row_idx[1] > 0)\n",
    "    .map(lambda row_idx: row_idx[0])\n",
    "    .take(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Transformation type: .reduceByKey()\n",
    "\n",
    "The reduceByKey(f) function reduces the elements of the RDD by using \"f\" as the key. So the \"f\" function shoule be commutative and associative in order to be computed correctly and in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ABE', 5113), ('ABI', 5128), ('ABQ', 64422), ('ABY', 1554), ('ACT', 392), ('ACV', 8429), ('ADQ', -254), ('AEX', 10193), ('AGS', 5003), ('ALB', 22362), ('ALO', 2866), ('AMA', 21979), ('ANC', 4948), ('ATL', 1151087), ('ATW', 8151), ('AUS', 108638), ('AVL', 5727), ('AVP', 2946), ('AZO', 233), ('BDL', 54662)]"
     ]
    }
   ],
   "source": [
    "# Determine the delays in regards to originating city. -> remove header row with zipWithIndex() and map()\n",
    "\n",
    "(\n",
    "    flights\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row_idx: row_idx[1] > 0)\n",
    "    .map(lambda row_idx: row_idx[0])\n",
    "    .map(lambda c: (c[3], int(c[1]) ) )\n",
    "    .reduceByKey(lambda x, y: x + y) # Take the count of the previous delay (x) and add it with the next delay count (y), gives total delays.\n",
    "    .take(20)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 Transformation type: .sortByKey()\n",
    "\n",
    "The sortByKey(asc) will order (key, value) pairs within the RDD by key and returns a transformed RDD in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ABE', 5113), ('ABI', 5128), ('ABQ', 64422), ('ABY', 1554), ('ACT', 392), ('ACV', 8429), ('ADQ', -254), ('AEX', 10193), ('AGS', 5003), ('ALB', 22362), ('ALO', 2866), ('AMA', 21979), ('ANC', 4948), ('ATL', 1151087), ('ATW', 8151), ('AUS', 108638), ('AVL', 5727), ('AVP', 2946), ('AZO', 233), ('BDL', 54662), ('BET', -645), ('BFL', 4022), ('BGR', 2852), ('BHM', 44355), ('BIL', 2616), ('BIS', 3825), ('BMI', 7817), ('BNA', 212243), ('BOI', 18004), ('BOS', 238602), ('BPT', 1936), ('BQK', 3952), ('BQN', 3943), ('BRO', 4967), ('BRW', 880), ('BTM', -138), ('BTR', 21989), ('BTV', 14755), ('BUF', 54309), ('BUR', 42241), ('BWI', 362845), ('BZN', 7226), ('CAE', 25686), ('CAK', 14749), ('CDC', 51), ('CDV', -1024), ('CEC', 2832), ('CHA', 7586), ('CHO', 2421), ('CHS', 30789)]"
     ]
    }
   ],
   "source": [
    "# Gets the origin code and delays, proceeds to remove the header, then runs a group by origin code via reduceByKey(), and finally sorting it by the key (origin code):\n",
    "(\n",
    "    flights\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row_idx: row_idx[1] > 0 ).map(lambda row_idx: row_idx[0])\n",
    "    .map(lambda c: (c[3], int(c[1])))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .sortByKey()\n",
    "    .take(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 Transformation type: .union()\n",
    "\n",
    "The union(RDD) returns a tranformed RDD that is the union of the source and argument RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make 'part_a' RDD of the Washington Airports:\n",
    "part_a = (\n",
    "    airports\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row_idx: row_idx[1] > 0)\n",
    "    .map(lambda row_idx: row_idx[0])\n",
    "    .filter(lambda c: c[1] == \"WA\")\n",
    ")\n",
    "\n",
    "# Make 'part_b' RDD of the British Columbia Airports:\n",
    "part_b = (\n",
    "    airports\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row_idx: row_idx[1] > 0)\n",
    "    .map(lambda row_idx: row_idx[0])\n",
    "    .filter(lambda c: c[1] == \"BC\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bellingham', 'WA', 'USA', 'BLI'], ['Moses Lake', 'WA', 'USA', 'MWH'], ['Pasco', 'WA', 'USA', 'PSC'], ['Pullman', 'WA', 'USA', 'PUW'], ['Seattle', 'WA', 'USA', 'SEA'], ['Spokane', 'WA', 'USA', 'GEG'], ['Walla Walla', 'WA', 'USA', 'ALW'], ['Wenatchee', 'WA', 'USA', 'EAT'], ['Yakima', 'WA', 'USA', 'YKM'], ['Abbotsford', 'BC', 'Canada', 'YXX'], ['Anahim Lake', 'BC', 'Canada', 'YAA'], ['Campbell River', 'BC', 'Canada', 'YBL'], ['Castlegar', 'BC', 'Canada', 'YCG'], ['Cranbrook', 'BC', 'Canada', 'YXC'], ['Fort Nelson', 'BC', 'Canada', 'YYE'], ['Fort Saint John', 'BC', 'Canada', 'YXJ'], ['Kamloops', 'BC', 'Canada', 'YKA'], ['Kelowna', 'BC', 'Canada', 'YLW'], ['Nanaimo', 'BC', 'Canada', 'YCD'], ['Penticton', 'BC', 'Canada', 'YYF'], ['Port Hardy', 'BC', 'Canada', 'YZT'], ['Powell River', 'BC', 'Canada', 'YPW'], ['Prince George', 'BC', 'Canada', 'YXS'], ['Prince Rupert', 'BC', 'Canada', 'YPR'], ['Quesnel', 'BC', 'Canada', 'YQZ'], ['\"Sandspit, Queen Charlotte Islands\"', 'BC', 'Canada', 'YZP'], ['Smithers', 'BC', 'Canada', 'YYD'], ['Terrace', 'BC', 'Canada', 'YXT'], ['Vancouver', 'BC', 'Canada', 'YVR'], ['Victoria', 'BC', 'Canada', 'YYJ'], ['Williams Lake', 'BC', 'Canada', 'YWL']]"
     ]
    }
   ],
   "source": [
    "# Perform the Union:\n",
    "part_a.union(part_b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.13 Transformation type: .mapPartitionsWithIndex()\n",
    "\n",
    "The mapPartitionsWithIndex(f) can be said to be similar to the map function, however it runs the \"f\" function separately on each of the partition and provides an index of the partition. \n",
    "\n",
    "This is useful when trying to determine the data skew within each of the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1391579]"
     ]
    }
   ],
   "source": [
    "# Uder defince Function: \n",
    "def partitionElementCount(idx, iterator):\n",
    "    count = 0\n",
    "    for _ in iterator:\n",
    "        count += 1\n",
    "    return idx, count\n",
    "\n",
    "# Apply the function above:\n",
    "flights.mapPartitionsWithIndex(partitionElementCount).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.14 Inspecting and understanding whats happening behind the scenes of a Spark Session:\n",
    "\n",
    "Every Spark session will launch a web-based UI and by default it would be on http://localhost:4040.\n",
    "Opening this link would show the following:\n",
    "\n",
    "- A list of scheduler stages and its tasks.\n",
    "- Summary of the RDD sizes and its memory usage.\n",
    "- Environmental Information.\n",
    "- Information about the running executors.\n",
    "\n",
    "Further information and details can be found on: https://spark.apache.org/docs/latest/monitoring.html.\n",
    "\n",
    "## 4.15 Example code for DAG visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ABE', ('01011245', 'PA')), ('ABE', ('01020600', 'PA')), ('ABE', ('01021245', 'PA')), ('ABE', ('01020605', 'PA')), ('ABE', ('01031245', 'PA'))]"
     ]
    }
   ],
   "source": [
    "flt = flights.map(lambda c: (c[3], c[0]))\n",
    "\n",
    "airp = airports.map(lambda c: (c[3], c[1]))\n",
    "\n",
    "flt.join(airp).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.16 DAG Visualisation:\n",
    "\n",
    "These visualisations below shows the map transformations. This is a single job (in this case Job 23) that have created 2 stages that are Stage 38 and Stage 39."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the required library:\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "os.chdir(path)\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'\n",
    "# print(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG Visual 1](Understanding DAG Visual 1.png \"DAG Visual 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a deeper look of what is happening: the image below.\n",
    "\n",
    "The task that was executed in the first stage (Stage 38) gives a DAG Visualisation and its Event Timeline too.\n",
    "\n",
    "- 1) The 2 textFile callouts were to extract the two different files that are departure_delays.csv and airport-codes.txt.\n",
    "- 2) Once the map functions were completed, it proceeds to support the join function where Spark executes the UnionRDD and PairwiseRDD to perform the join as part of the union task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG Visual 2](Understanding DAG Visual 2.png \"DAG Visual 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next stage: \n",
    "\n",
    "The partitionBy and mapPartitions tasks would then shuffle and re-map the partitions before proceeding to give an output by using the take() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG Visual 3](Understanding DAG Visual 3.png \"DAG Visual 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 More on RDD Actions:\n",
    "\n",
    "An RDD Action would return aa value to the driver only after performing the computation on the dataset. These are typically done on the worker nodes.\n",
    "\n",
    "\n",
    "## 5.1 Action type: .take()\n",
    "\n",
    "This .take() function will return an arrau with the first \"n\" elements stated of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['City', 'State', 'Country', 'IATA'], ['Abbotsford', 'BC', 'Canada', 'YXX'], ['Aberdeen', 'SD', 'USA', 'ABR'], ['Abilene', 'TX', 'USA', 'ABI'], ['Akron', 'OH', 'USA', 'CAK'], ['Alamosa', 'CO', 'USA', 'ALS'], ['Albany', 'GA', 'USA', 'ABY'], ['Albany', 'NY', 'USA', 'ALB'], ['Albuquerque', 'NM', 'USA', 'ABQ'], ['Alexandria', 'LA', 'USA', 'AEX'], ['Allentown', 'PA', 'USA', 'ABE'], ['Alliance', 'NE', 'USA', 'AIA'], ['Alpena', 'MI', 'USA', 'APN'], ['Altoona', 'PA', 'USA', 'AOO'], ['Amarillo', 'TX', 'USA', 'AMA'], ['Anahim Lake', 'BC', 'Canada', 'YAA'], ['Anchorage', 'AK', 'USA', 'ANC'], ['Appleton', 'WI', 'USA', 'ATW'], ['Arviat', 'NWT', 'Canada', 'YEK'], ['Asheville', 'NC', 'USA', 'AVL'], ['Aspen', 'CO', 'USA', 'ASE'], ['Athens', 'GA', 'USA', 'AHN'], ['Atlanta', 'GA', 'USA', 'ATL'], ['Atlantic City', 'NJ', 'USA', 'ACY'], ['Augusta', 'GA', 'USA', 'AGS'], ['Augusta', 'ME', 'USA', 'AUG'], ['Austin', 'TX', 'USA', 'AUS'], ['Bagotville', 'PQ', 'Canada', 'YBG'], ['Baie-Comeau', 'PQ', 'Canada', 'YBC'], ['Bakersfield', 'CA', 'USA', 'BFL'], ['Baltimore', 'MD', 'USA', 'BWI'], ['Bangor', 'ME', 'USA', 'BGR'], ['Bar Harbor', 'ME', 'USA', 'BHB'], ['Barrow', 'AK', 'USA', 'BRW'], ['Baton Rouge', 'LA', 'USA', 'BTR'], ['Beaumont', 'TX', 'USA', 'BPT'], ['Beckley', 'WV', 'USA', 'BKW'], ['Bedford', 'MA', 'USA', 'BED'], ['Bellingham', 'WA', 'USA', 'BLI'], ['Bemidji', 'MN', 'USA', 'BJI'], ['Bethel', 'AK', 'USA', 'BET'], ['Bettles', 'AK', 'USA', 'BTT'], ['Billings', 'MT', 'USA', 'BIL'], ['Binghamton', 'NY', 'USA', 'BGM'], ['Birmingham', 'AL', 'USA', 'BHM'], ['Bismarck', 'ND', 'USA', 'BIS'], ['Bloomington', 'IL', 'USA', 'BMI'], ['Bloomington', 'IN', 'USA', 'BMG'], ['Bluefield', 'WV', 'USA', 'BLF'], ['Boise', 'ID', 'USA', 'BOI']]"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "airports.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Action type: .collect()\n",
    "\n",
    "This .collect() function will return all of the lements from the workers and drivers.\n",
    "\n",
    "NOTE: Depending on the cluster setup and the size of the dataset, this can take a very long time as well as computationally taxing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bellingham', 'WA', 'USA', 'BLI'], ['Moses Lake', 'WA', 'USA', 'MWH'], ['Pasco', 'WA', 'USA', 'PSC'], ['Pullman', 'WA', 'USA', 'PUW'], ['Seattle', 'WA', 'USA', 'SEA'], ['Spokane', 'WA', 'USA', 'GEG'], ['Walla Walla', 'WA', 'USA', 'ALW'], ['Wenatchee', 'WA', 'USA', 'EAT'], ['Yakima', 'WA', 'USA', 'YKM']]"
     ]
    }
   ],
   "source": [
    "# Return all the airports' elements: filter it by WA State.\n",
    "airports.filter(lambda c: c[1] == \"WA\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Action type: .reduce()\n",
    "\n",
    "The .reduce(f) action function would aggregate the elements of the RDD according to the function \"f\". This \"f\" function should therefore be commutative and associative so that it can be computed correctly and in parallel.\n",
    "\n",
    "NOTE: this func. needs to be both associative and commutative, where for example, if there is a change in the order of elements, the operands does not change the results.\n",
    "\n",
    "Source: \n",
    "- https://sciencing.com/associative-commutative-property-of-addition-multiplication-with-examples-13712459.html\n",
    "- https://www.purplemath.com/modules/numbprop.htm\n",
    "\n",
    "EXAMPLE: \n",
    "\n",
    "Associativity Rule -> (6 + 3) + 4 = 6 + (3 + 4)\n",
    "Commutative Rule -> 6 + 3 + 4 = 4 + 3 + 6\n",
    "\n",
    "EXAMPLE 2: \n",
    "- The RDD (with one partition only!) is: \n",
    "\n",
    "data_example = sc.parallelize([1, 2, 0.5, 0.1, 5, 0.2], 1) \n",
    "\n",
    "And reducing data to divide the current result by the subsequent one, we would expect a value of 10: \\\n",
    "works = data_example.reduce(lambda x, y: x / y) \\\n",
    "works = 10\n",
    "\n",
    "- Partitioning the data into three partitions will produce an incorrect result: \n",
    "\n",
    "data_example = sc.parallelize([1, 2, .5, .1, 5, .2], 3) \\\n",
    "data_example.reduce(lambda x, y: x / y) \\\n",
    "output -> 0.004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22293"
     ]
    }
   ],
   "source": [
    "# Find the total flight delays: check between SEA (origin) and SFO (dest), next is to convert these delays column to integers and summarise them.\n",
    "\n",
    "(\n",
    "    flights\n",
    "    .filter(lambda c: c[3] == \"SEA\" and c[4] == \"SFO\")\n",
    "    .map(lambda c: int(c[1]))\n",
    "    .reduce(lambda x, y: x + y)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Action type: .count()\n",
    "\n",
    "The count() action function would return the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391578"
     ]
    }
   ],
   "source": [
    "(\n",
    "    flights\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row_idx: row_idx[1] > 0)\n",
    "    .map(lambda row_idx: row_idx[0])\n",
    "    .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Action type: .saveAsTextFile()\n",
    "\n",
    "This action will save the RDD into a Text File.\n",
    "\n",
    "NOTE: the data on each of the partition will be saved into a separate file. This can be seen as:\n",
    "- /Temp_Save/airports/_SUCCESS\n",
    "- /Temp_Save/airports/part-00000\n",
    "- /Temp_Save/airports/part-00001\n",
    "\n",
    "etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savepath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saves the data into a Text File:\n",
    "\n",
    "airports.saveAsTextFile(savepath + \"/Temp_Save/airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Once again, understanding whats happening behind the scenes of a Spark Session:\n",
    "\n",
    "Every Spark session will launch a web-based UI and by default it would be on http://localhost:4040.\n",
    "Opening this link would show the following:\n",
    "\n",
    "- A list of scheduler stages and its tasks.\n",
    "- Summary of the RDD sizes and its memory usage.\n",
    "- Environmental Information.\n",
    "- Information about the running executors.\n",
    "\n",
    "Further information and details can be found on: https://spark.apache.org/docs/latest/monitoring.html.\n",
    "\n",
    "## 5.7 Example code for DAG visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ABE', 5113), ('ABI', 5128), ('ABQ', 64422), ('ABY', 1554), ('ACT', 392)]"
     ]
    }
   ],
   "source": [
    "(\n",
    "    flights\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row_idx: row_idx[1] > 0)\n",
    "    .map(lambda row_idx: row_idx[0])\n",
    "    .map(lambda c: (c[3], int(c[1])))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 DAG Visualisation:\n",
    "\n",
    "These visualisations below shows the reduceByKey() action. This is a single job (in this case Job ID 25) that represents only the reduceByKey() part of the DAG as the previous .zipWithIndex() transformation is not included in this Job ID 25. This can be verified by taking a look at the \"Completed Stages\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the required library:\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "# Set up the working directory for the images:\n",
    "os.chdir(path)\n",
    "image_folderName = 'Description Images'\n",
    "image_path = os.path.abspath(image_folderName) + '/'\n",
    "# print(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG Visual 4](Understanding DAG Visual 1.png \"DAG Visual 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a deeper look of what is happening: the image below.\n",
    "\n",
    "The bulk of the task was executed in the first stage (Stage 30) where it extracts the data from the departure_delays.csv file and proceeds to reduceByKey(). This section can be made faster with greater partitions than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG Visual 5](Understanding DAG Visual 2.png \"DAG Visual 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next stage: \n",
    "\n",
    "The partitionBy and mapPartitions tasks would then shuffle and re-map the partition(s) before proceeding to give an output by using the take() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG Visual 6](Understanding DAG Visual 3.png \"DAG Visual 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Disadvantages of using RDD:\n",
    "\n",
    "In general, RDDs allows for higher flexibility like performing functional operators such as map, reduce and shuffle provding a wide variety of possible transformations on the data. \n",
    "\n",
    "__One issue is__: avoid using GroupByKey on larger datasets.\n",
    "- where it incurs a lot of unnessary data to being transferred over the network and may cause out of disk problems.\n",
    "\n",
    "ref: https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/avoid_groupbykey_when_performing_an_associative_re/avoid-groupbykey-when-performing-an-associative-reductiove-operation\n",
    "\n",
    "__Another is__: RDDs are slow.\n",
    "- because when a PySpark program executes using RDDs it will incur a large overhead to execute the job. This happens due to a lot of context switching amd communications overhead betwen Python and JVM when PySpark is trying to distribute the processing on multiple workers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
