{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstracting Data with DataFrames\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This notebook aims to focus on furthering the understanding of PySpark fundamentals in data structures called DataFrames. Interestingly, DataFrames in PySpark takes advantage of the developments and improvements from Project Tungsten and Catalyst Optimiser.\n",
    "\n",
    "## Project Tungsten:\n",
    "\n",
    "The following is a direct quote from https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html. It briefly describes the project. \n",
    "\n",
    "Project Tungsten will be the largest change to Spark’s execution engine since the project’s inception. It focuses on substantially improving the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of modern hardware. This effort includes three initiatives:\n",
    "\n",
    "- __Memory Management and Binary Processing__: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection.\n",
    "- __Cache-aware computation__: algorithms and data structures to exploit memory hierarchy.\n",
    "- __Code generation__: using code generation to exploit modern compilers and CPUs.\n",
    "\n",
    "## Catalyst Optimiser:\n",
    "\n",
    "The following is a direct quote from https://databricks.com/glossary/catalyst-optimizer. It briefly describes the project.\n",
    "\n",
    "Catalyst is based on functional programming constructs in Scala and designed with these key two purposes:\n",
    "\n",
    "- Easily add new optimization techniques and features to Spark SQL.\n",
    "- Enable external developers to extend the optimizer (e.g. adding data source specific rules, support for new data types, etc.).\n",
    "\n",
    "\n",
    "## Breakdown of this Notebook:\n",
    "- Creating DataFrames\n",
    "- Accessing underlying RDDs\n",
    "- Performance optimisations\n",
    "- Inferring the schema using Reflections\n",
    "- Specifying the schema programmatically\n",
    "- Creating a temporary Table\n",
    "- Using SQL to interact with DataFrames\n",
    "- Overview of the DataFrame transformations and Actions.\n",
    "\n",
    "## 1 PySpark Machine Configuration:\n",
    "\n",
    "Here it only uses two processing cores from the CPU, and it set up by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"executorCores\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setup the Correct Directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the Path:\n",
    "path = '++++your working directory here++++/Datasets/'\n",
    "os.chdir(path)\n",
    "folder_pathway = os.getcwd()\n",
    "\n",
    "# print(folder_pathway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Creating DataFrames:\n",
    "\n",
    "First is to create some sample data and column headers as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = sc.parallelize(\n",
    "    [(1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD', 13.75, 9.48, 0.61, 4.02), \n",
    "    (2, 'MacBook', 2016, '12\"', '8GB', '256GB SSD', 11.04, 7.74, 0.52, 2.03), \n",
    "    (3, 'MacBook Air', 2016, '13.3\"', '8GB', '128GB SSD', 12.8, 8.94, 0.68, 2.96), \n",
    "    (4, 'iMac', 2017, '27\"', '64GB', '1TB SSD', 25.6, 8.0, 20.3, 20.8)]\n",
    ")\n",
    "\n",
    "col_names = ['Id', 'Model', 'Year', 'ScreenSize', 'RAM', 'HDD', 'W', 'D', 'H', 'Weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataFrame from the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_df = spark.createDataFrame(sample_data, col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Id=1, Model='MacBook Pro', Year=2015, ScreenSize='15\"', RAM='16GB', HDD='512GB SSD', W=13.75, D=9.48, H=0.61, Weight=4.02), Row(Id=2, Model='MacBook', Year=2016, ScreenSize='12\"', RAM='8GB', HDD='256GB SSD', W=11.04, D=7.74, H=0.52, Weight=2.03)]"
     ]
    }
   ],
   "source": [
    "# Inspect:\n",
    "sample_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it can be seen, unlike RDDs, a DataFrame is a collection of Row(.......) objects and that a Row(......) object consists of data that is named.\n",
    "\n",
    "## 3.1 To have a look at the DataFrame Data:\n",
    "\n",
    "This can be done by using the .show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+"
     ]
    }
   ],
   "source": [
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 To have a look at the Schema:\n",
    "\n",
    "As DataFrames have a schema, and that the columns of the DataFrame have matching datatypes as the original sample_data RDD.\n",
    "\n",
    "It can be examined like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "sample_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create DataFrame from a JSON file:\n",
    "\n",
    "#### From an example source.\n",
    "\n",
    "source: https://github.com/kavgan/spark-examples/blob/master/sample-data/description.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data_json_df = ( spark.read.json(folder_pathway + '/Datasets/' + 'description.json') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "sample_data_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                text|      title|\n",
      "+--------------------+-----------+\n",
      "|Data (/ˈdeɪtə/ DA...|       Data|\n",
      "|Big data is a ter...|   Big Data|\n",
      "|Natural language ...|        NLP|\n",
      "|Text mining, also...|Text Mining|\n",
      "+--------------------+-----------+"
     ]
    }
   ],
   "source": [
    "sample_data_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From sample_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data_json_df = ( spark.read.json(folder_pathway + '/Datasets/' + 'sample_data.json') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |--  D: double (nullable = true)\n",
      " |--  H: double (nullable = true)\n",
      " |--  HDD: string (nullable = true)\n",
      " |--  Model: string (nullable = true)\n",
      " |--  RAM: string (nullable = true)\n",
      " |--  ScreenSize: string (nullable = true)\n",
      " |--  W: double (nullable = true)\n",
      " |--  Weight: double (nullable = true)\n",
      " |--  Year: long (nullable = true)\n",
      " |-- Id: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "sample_data_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+-----------+----+-----------+-----+-------+-----+---+\n",
      "|   D|   H|      HDD|      Model| RAM| ScreenSize|    W| Weight| Year| Id|\n",
      "+----+----+---------+-----------+----+-----------+-----+-------+-----+---+\n",
      "|9.48|0.61|512GB SSD|MacBook Pro|16GB|        15\"|13.75|   4.02| 2015|  1|\n",
      "|7.74|0.52|256GB SSD|    MacBook| 8GB|        12\"|11.04|   2.03| 2016|  2|\n",
      "|8.94|0.68|128GB SSD|MacBook Air| 8GB|      13.3\"| 12.8|   2.96| 2016|  3|\n",
      "| 8.0|20.3|  1TB SSD|       iMac|64GB|        27\"| 25.6|   20.8| 2017|  4|\n",
      "+----+----+---------+-----------+----+-----------+-----+-------+-----+---+"
     ]
    }
   ],
   "source": [
    "sample_data_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create a DataFrame from CSV file:\n",
    "\n",
    "Unlike loading it in from a JSON file, a CSV file here requires more parameters. These paramters are \"header\" and \"inferSchema\". The header parameter will try to assign the right data-type to each column, and the inderSchema parameter will assign strings as the default.\n",
    "\n",
    "#### From an example source:\n",
    "\n",
    "Source: https://github.com/kavgan/spark-examples/blob/master/sample-data/description.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data_csv_df = ( spark.read.csv(folder_pathway + '/Datasets/' + 'description.csv', \n",
    "                                       header=True, inferSchema=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "sample_data_csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|      title|                text|\n",
      "+-----------+--------------------+\n",
      "|       Data|Data (/ˈdeɪtə/ DA...|\n",
      "|   Big Data|Big data is a ter...|\n",
      "|        NLP|                text|\n",
      "|Text Mining|Text mining, also...|\n",
      "+-----------+--------------------+"
     ]
    }
   ],
   "source": [
    "sample_data_csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From sample_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data_csv_df = ( spark.read.csv(folder_pathway + '/Datasets/' + 'sample_data.csv', \n",
    "                                       header=True, inferSchema=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |--  Model: string (nullable = true)\n",
      " |--  Year: integer (nullable = true)\n",
      " |--  ScreenSize: string (nullable = true)\n",
      " |--  RAM: string (nullable = true)\n",
      " |--  HDD: string (nullable = true)\n",
      " |--  W: double (nullable = true)\n",
      " |--  D: double (nullable = true)\n",
      " |--  H: double (nullable = true)\n",
      " |--  Weight: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "sample_data_csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+-----------+----+---------+-----+----+----+-------+\n",
      "| Id|      Model| Year| ScreenSize| RAM|      HDD|    W|   D|   H| Weight|\n",
      "+---+-----------+-----+-----------+----+---------+-----+----+----+-------+\n",
      "|  1|MacBook Pro| 2015|        15\"|16GB|512GB SSD|13.75|9.48|0.61|   4.02|\n",
      "|  2|    MacBook| 2016|        12\"| 8GB|256GB SSD|11.04|7.74|0.52|   2.03|\n",
      "|  3|MacBook Air| 2016|      13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|   2.96|\n",
      "|  4|       iMac| 2017|        27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|   20.8|\n",
      "+---+-----------+-----+-----------+----+---------+-----+----+----+-------+"
     ]
    }
   ],
   "source": [
    "sample_data_csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Accessing the underlying RDDs:\n",
    "\n",
    "DataFrames under the hood does continue to use RDDs. This section will describe the process of interacting with the underlying RDD of a DataFrame.\n",
    "\n",
    "## 4.1 Import the Required Libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql as sql\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Extract the Size and Type of the HDD:\n",
    "\n",
    "These will be represented in separate columns. The minimum volume needed will be calculated for each computer specifications and put into boxes. \n",
    "\n",
    "### To do this:\n",
    "1 -> Begin by extracting the \".rdd\" from the sample_df. \\\n",
    "2 -> Use the \".map()\" transformation to ass the HDD_size column to the schema.\n",
    "3 -> As working with RDD, there is a need to retain all the other columns, so these are converted into row objects with the \"Row()\" function and then into a dictionary by using \".asDict()\" method. This is so that it can be later unpacked using the \" ** \". \n",
    "\n",
    "    NOTE in Python: \n",
    "    - A single * preceding a list of tuples is passed as a parameter to a function, it then passes each element of the list as a separate argument to the function. \n",
    "    - Whereas the double ** will take the first element and turn it into a keyword parameter and the second element will be the value to be passed. \n",
    "    \n",
    "\n",
    "4 -> Inside this Row(), the second argument is where the name of the column \"HDD_size\" is passed and set to the desired value. Here, the \".HDD\" column is split and the first element is extracted as it is the HDD_size. \\\n",
    "5 -> The same is done for the \"HDD_type\" column. \\\n",
    "6 -> The 3rd \".map()\" function adds the Volume information and follows the same method as the last two steps (Steps 4+5)\\\n",
    "7 -> The \".toDF()\" method is used to convert the RDD back into the DataFrame. \\\n",
    "8 -> To ensure that the column names and the row data of the DataFrame is not empty, the \".select()\" function is used to select the relevant information. Additionally, the Volume column is rounded with the \".round()\" function and the Cubic Inch volume is created using the \".alias()\" methid.\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformed_dat_sample = (\n",
    "    sample_df\n",
    "    .rdd\n",
    "    .map(lambda row: sql.Row(**row.asDict(), HDD_size = row.HDD.split(' ')[0])\n",
    "        ).map(lambda row: sql.Row(**row.asDict(), HDD_type = row.HDD.split(' ')[1])\n",
    "             ).map(lambda row: sql.Row(**row.asDict(), Volume = row.H * row.D * row.W)\n",
    "                  )\n",
    "    .toDF()\n",
    "    .select(\n",
    "        sample_df.columns + ['HDD_size', 'HDD_type', f.round(f.col('Volume')).alias('Volume_cuIn')]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Id=1, Model='MacBook Pro', Year=2015, ScreenSize='15\"', RAM='16GB', HDD='512GB SSD', W=13.75, D=9.48, H=0.61, Weight=4.02, HDD_size='512GB', HDD_type='SSD', Volume_cuIn=80.0)]"
     ]
    }
   ],
   "source": [
    "# Inspect the RDD under the hood:\n",
    "transformed_dat_sample.rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it can be seen above, the \"transformed_dat_sample\" produces a single item list that consist of the element Row(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD', 13.75, 9.48, 0.61, 4.02)]"
     ]
    }
   ],
   "source": [
    "# Inspect the DataFrame:\n",
    "sample_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it can be seen above, the \"sample_data\" produces a single item list, however, the item is now a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+--------+--------+-----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|HDD_size|HDD_type|Volume_cuIn|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+--------+--------+-----------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|   512GB|     SSD|       80.0|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|   256GB|     SSD|       44.0|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|   128GB|     SSD|       78.0|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|     1TB|     SSD|     4157.0|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+--------+--------+-----------+"
     ]
    }
   ],
   "source": [
    "# Show the whole DataFrame:\n",
    "transformed_dat_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Performance Optimisations on User Defined Functions (UDFs):\n",
    "\n",
    "DataFrames on PySpark did have performance improvements, however, when it came to using User Define Functions, PySpark would constantly switch runtimes between Python and JVM and resulted in a great performance hit. \n",
    "\n",
    "This was fixed when Spark adapted the improvements from project Arrow (https://arrow.apache.org). This was where a single memory space was created and used by all environments and relieved Spark from the constant need to copying and converting between objects. More details can be found from the above mentioned link.\n",
    "\n",
    "## 5.1 Import the required Libraries:\n",
    "\n",
    "NOTE: \n",
    "- pip install pyarrow\n",
    "- may require a different version. Try -> pip install pyarrow==0.14.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f \n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# setup to work around with pandas udf\n",
    "# see answers here https://stackoverflow.com/questions/58458415/pandas-scalar-udf-failing-illegalargumentexception\n",
    "os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Demonstration:\n",
    "\n",
    "### What is happening here?\n",
    "\n",
    "1 -> Firstly, create a Spark dataFrame that has a range from 0 to 1 million, with the column name as \"val\". These values should be randomise, by using \".rand()\". \\\n",
    "2 -> Cache the dataFrame with the \".cache()\" method. \\\n",
    "3 -> Show the contents of the DataFrame with the \".show()\" method. \\\n",
    "4 -> Secondly, define the \"pandas_cdf(...)\" method. This declares that it is using a vectorised UDF in PySpark. The first parameter is set to \"double\". Note that this can be either a DDL-formatted type string or a pyspark.sqp.types.DataType. The second parameter is a function type, and to set it to return a single column, the \".PandasUDFType.SCALAR\" is used. If the operation is required for multiple columns, it would be set as \".PandasUDFType.GROUPED_MAP\". \\\n",
    "5 -> Next, is the UDF of \"pandas_pdf()\" function where it takes in a single column and returns a pandas.Series object. The values of the normal CDF numbers. \\\n",
    "6 -> Finally, the dataFrame is transformed by use of the UDF with a new column name and shown with the \".show()\" method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                val|\n",
      "+---+-------------------+\n",
      "|  0|0.08198988133925589|\n",
      "|  1| 0.8509392124864215|\n",
      "|  2| 0.4077124736653549|\n",
      "+---+-------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "# Make a BIG dataFrame:\n",
    "df_big = (\n",
    "    spark\n",
    "    .range(0, 1000000)\n",
    "    .withColumn('val', f.rand())\n",
    ")\n",
    "\n",
    "# Cache it:\n",
    "df_big.cache()\n",
    "\n",
    "# Inspect:\n",
    "df_big.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Declaration:\n",
    "@f.pandas_udf('double', f.PandasUDFType.SCALAR)\n",
    "\n",
    "# User Define Function:\n",
    "def pandas_pdf(v):\n",
    "    return pd.Series(stats.norm.pdf(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the UDF:\n",
    "# (\n",
    "#     df_big\n",
    "#     .withColumn('probability', pandas_pdf(df_big.val))\n",
    "#     .show(5)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Compare the performance of the approaches:\n",
    "\n",
    "#### Test 1 - Vectorised pandas UDF\n",
    "\n",
    "The \"test_pandas_pdf()\" uses the pandas_pdf() to retrieve the PDF from the normal distribution where it then performs a \".count()\" operation and prints out the results by using the \".show()\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(probability)|\n",
      "+------------------+\n",
      "|           1000000|\n",
      "+------------------+\n",
      "\n",
      "1.5764408579999998"
     ]
    }
   ],
   "source": [
    "def test_pandas_pdf():\n",
    "    return (\n",
    "        df_big\n",
    "        .withColumn('probability', pandas_pdf(df_big.val))\n",
    "        .agg(f.count(f.col('probability')))\n",
    "        .show()\n",
    "    )\n",
    "\n",
    "# Run and time the function:\n",
    "# %timeit -n 1 test_pandas_pdf()\n",
    "\n",
    "print(timeit.timeit(test_pandas_pdf, number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2 - Row-byRow version with Python to JVM conversion.\n",
    "\n",
    "The test_pdf() method will be similar but instead, uses the \"pdf()\" method to perform a row-by-row compute version of the UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(probability)|\n",
      "+------------------+\n",
      "|           1000000|\n",
      "+------------------+\n",
      "\n",
      "77.427466586"
     ]
    }
   ],
   "source": [
    "# Declaration:\n",
    "@f.udf('double')\n",
    "\n",
    "# UDF:\n",
    "def pdf(v):\n",
    "    return float(stats.norm.pdf(v))\n",
    "\n",
    "#\n",
    "def test_pdf():\n",
    "    return(\n",
    "        df_big\n",
    "        .withColumn('probability', pdf(df_big.val))\n",
    "        .agg(f.count(f.col('probability')))\n",
    "        .show()\n",
    "    )\n",
    "\n",
    "\n",
    "# Run and time the function:\n",
    "# %timeit -n1 test_pdf()\n",
    "\n",
    "print(timeit.timeit(test_pdf, number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Immediately, it can be seen that the vectoried UDF method with Pandas is much faster at 3 seconds over the row-by-row method that took 428 seconds. This is performance boost from project Arrow.\n",
    "\n",
    "## 6 Inferring the schema utilising Reflection:\n",
    "\n",
    "It is also important to note that RDDs does not have schema and the DataFrames do have it. This section will create a DataFrame by inferring the schema by using Reflection.\n",
    "\n",
    "## 6.1 Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Read in the CSV sample data :\n",
    "\n",
    "After reading in the CSV data, it will be saved and created as an RDD which will then be used to to create a DataFrame.\n",
    "\n",
    "source: https://spark.apache.org/docs/2.3.0/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the data:\n",
    "sample_data_RDD = sc.textFile(folder_pathway + '/Datasets/' + 'sample_data.csv')\n",
    "\n",
    "# Define the column header: .first() here references to the first row of the data.\n",
    "header = sample_data_RDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "sample_data_RDD_row = (\n",
    "    sample_data_RDD\n",
    "    .filter(lambda row: row != header)\n",
    "    .map(lambda row: row.split(','))\n",
    "    .map(lambda row: \n",
    "        sql.Row(\n",
    "            Id = int(row[0]),\n",
    "            Model = row[1],\n",
    "            Year = row[2],\n",
    "            ScreenSize = row[3],\n",
    "            RAM = row[4],\n",
    "            HDD = row[5],\n",
    "            W = float(row[6]),\n",
    "            D = float(row[7]),\n",
    "            H = float(row[8]),\n",
    "            Weight = float(row[9])\n",
    "        ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(D=9.48, H=0.61, HDD='512GB SSD', Id=1, Model='MacBook Pro', RAM='16GB', ScreenSize='\"15\\\\\"\"', W=13.75, Weight=4.02, Year='2015')]"
     ]
    }
   ],
   "source": [
    "# Inspect the RDD:\n",
    "sample_data_RDD_row.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|    \"15\\\"\"|13.75|  4.02|2015|\n",
      "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|    \"12\\\"\"|11.04|  2.03|2016|\n",
      "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|  \"13.3\\\"\"| 12.8|  2.96|2016|\n",
      "| 8.0|20.3|  1TB SSD|  4|       iMac|64GB|    \"27\\\"\"| 25.6|  20.8|2017|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+"
     ]
    }
   ],
   "source": [
    "# Inspect the DataFrame:\n",
    "sample_data_df = spark.createDataFrame(sample_data_RDD_row)\n",
    "sample_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Specifying the Schema Programmatically:\n",
    "\n",
    "## 7.1 Import the required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Create the schema:\n",
    "\n",
    "Below will show the following:\n",
    "- \".StructField()\" is the programmatic way of adding a field to a schema in PySpark. Here the first parameter is the nameof the column, and the second parameter is the dataType of the data for this column. The last parameter is a boolean that defines if the values for this column can contain null values (True) or no null values (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema_pgm = typ.StructType(\n",
    "    [typ.StructField('Id', typ.LongType(), False),\n",
    "    typ.StructField('Model', typ.StringType(), True),\n",
    "    typ.StructField('Year', typ.IntegerType(), True),\n",
    "    typ.StructField('ScreenSize', typ.StringType(), True),\n",
    "    typ.StructField('RAM', typ.StringType(), True),\n",
    "    typ.StructField('HDD', typ.StringType(), True),\n",
    "    typ.StructField('W', typ.DoubleType(), True),\n",
    "    typ.StructField('D', typ.DoubleType(), True),\n",
    "    typ.StructField('H', typ.DoubleType(), True),\n",
    "    typ.StructField('Weight', typ.DoubleType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Load in the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data_RDD = sc.textFile(folder_pathway + '/Datasets/' + 'sample_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Create the underlying RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the headers:\n",
    "header = sample_data_RDD.first()\n",
    "\n",
    "# RDD:\n",
    "sample_data_rdd = (\n",
    "    sample_data_RDD\n",
    "    .filter(lambda row: row != header)\n",
    "    .map(lambda row: row.split(','))\n",
    "    .map(lambda row: (\n",
    "        int(row[0]),\n",
    "        row[1],\n",
    "        int(row[2]),\n",
    "        row[3],\n",
    "        row[4],\n",
    "        row[5],\n",
    "        float(row[6]),\n",
    "        float(row[7]),\n",
    "        float(row[8]),\n",
    "        float(row[9]),\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the RDD format into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+"
     ]
    }
   ],
   "source": [
    "sample_data_schema_DF = spark.createDataFrame(sample_data_rdd, schema = schema_pgm)\n",
    "\n",
    "# Inspect:\n",
    "sample_data_schema_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Creating a Temporary Table:\n",
    "\n",
    "PySpark DataFrame can also be manipulated with SQL queries. This section will explore the creation of a temporary view to access the data inside the DataFrame through SQL queries.\n",
    "\n",
    "This example will also be using the \"sample_data_schema_DF\" from the previous section.\n",
    "\n",
    "## 8.1 Create the temp view:\n",
    "\n",
    "This method creates a temporary view that allows for querying the data and requires only 1 parameter that is the name of the view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data_schema_DF.createTempView('sample_data_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+---------+\n",
      "|      Model|Year| RAM|      HDD|\n",
      "+-----------+----+----+---------+\n",
      "|MacBook Pro|2015|16GB|512GB SSD|\n",
      "|    MacBook|2016| 8GB|256GB SSD|\n",
      "|MacBook Air|2016| 8GB|128GB SSD|\n",
      "|       iMac|2017|64GB|  1TB SSD|\n",
      "+-----------+----+----+---------+"
     ]
    }
   ],
   "source": [
    "# 8.2 Use the temp view to extract data:\n",
    "spark.sql('''\n",
    "    SELECT Model, Year, RAM, HDD FROM sample_data_view\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Note: Update a View:\n",
    "\n",
    "Once a temp view is created, it is not possible to create another view with the same name. However, Spark does provide another method which allows for the creation and update of a view.\n",
    "\n",
    "This is called \".createOrReplaceTempView()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data_schema_DF.createOrReplaceTempView('sample_data_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+---------+----------+\n",
      "|      Model|Year| RAM|      HDD|ScreenSize|\n",
      "+-----------+----+----+---------+----------+\n",
      "|MacBook Pro|2015|16GB|512GB SSD|    \"15\\\"\"|\n",
      "|    MacBook|2016| 8GB|256GB SSD|    \"12\\\"\"|\n",
      "|MacBook Air|2016| 8GB|128GB SSD|  \"13.3\\\"\"|\n",
      "|       iMac|2017|64GB|  1TB SSD|    \"27\\\"\"|\n",
      "+-----------+----+----+---------+----------+"
     ]
    }
   ],
   "source": [
    "# Use this NEW temp view to extract the data:\n",
    "spark.sql('''\n",
    "    SELECT Model, Year, RAM, HDD, ScreenSize FROM sample_data_view\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Utilising SQL to interact with the DataFrames:\n",
    "\n",
    "This section will explore the data within the DataFrame using the SQL queries.\n",
    "\n",
    "## 9.1 Extend the data of the original Data:\n",
    "\n",
    "Here, the orignal DataFrame is extended with the form factor for each of the computer model.\n",
    "\n",
    "To do this:\n",
    "- Create a DataFrame with two columns that are \"Model\" and \"FormFactor\", the RDD is converted to DataFrame with the \".toDF()\" method. The list passed here are the column names and the schema which will be automatically inferred.\n",
    "- Create the model's view and replace the \"sample_data_view\".\n",
    "- Append the \"FormFactor\" onto the original DataFrame by joining both the Views via the \"Model\" column. Within the \".sql()\" function, it can be seen below that it takes in regular SQL expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the FormFactor:\n",
    "models_df = sc.parallelize([\n",
    "    ('MacBook Pro', 'Laptop'),\n",
    "    ('MacBook', 'Laptop'),\n",
    "    ('MacBook Air', 'Laptop'),\n",
    "    ('iMac', 'Desktop')\n",
    "]).toDF(['Model', 'FormFactor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a temp View(s):\n",
    "models_df.createOrReplaceTempView('models')\n",
    "\n",
    "sample_data_schema_DF.createOrReplaceTempView('sample_data_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|FormFactor|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|   Desktop|\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|    Laptop|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|    Laptop|\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|    Laptop|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+"
     ]
    }
   ],
   "source": [
    "# SQL Query:\n",
    "spark.sql('''\n",
    "    SELECT a.*, b.FormFactor \n",
    "    FROM sample_data_view AS a \n",
    "    LEFT JOIN models AS b \n",
    "        ON a.Model == b.Model\n",
    "    ORDER BY Weight DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Perform some aggregations on the Data:\n",
    "\n",
    "Count the number of devices that are of different FormFactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FormFactor|ComputerCnt|\n",
      "+----------+-----------+\n",
      "|    Laptop|          3|\n",
      "|   Desktop|          1|\n",
      "+----------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT b.FormFactor, COUNT(*) AS ComputerCnt \n",
    "    FROM sample_data_view AS a \n",
    "    LEFT JOIN models AS b \n",
    "        ON a.Model == b.Model \n",
    "    GROUP BY FormFactor\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Overview of the DataFrame Transformations:\n",
    "\n",
    "Similar to RDDs, DataFrames (DF) in PySpark also have transformations and actions.  \n",
    "\n",
    "## 10.1 DataFrame Transformation Type: .select()\n",
    "\n",
    "The .select() function transforms to a new DF to extract the column(s) from the DF. Below shows the code.\n",
    "\n",
    "Additionally, there is a similar SQL syntax that does the same thing:\n",
    "\n",
    "SELECT Model, ScreenSize FROM sample_data_schema_DF;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|      Model|ScreenSize|\n",
      "+-----------+----------+\n",
      "|MacBook Pro|    \"15\\\"\"|\n",
      "|    MacBook|    \"12\\\"\"|\n",
      "|MacBook Air|  \"13.3\\\"\"|\n",
      "|       iMac|    \"27\\\"\"|\n",
      "+-----------+----------+"
     ]
    }
   ],
   "source": [
    "sample_data_schema_DF.select('Model', 'ScreenSize').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 DataFrame Transformation Type:  .filter()\n",
    "\n",
    "The .filter() function will transform a new DF where it only selects the row that are passed as the condition specified. In SQL syntax, it would be similar to \"WHERE\".\n",
    "\n",
    "Additionally, there is a similar SQL syntax that does the same thing:\n",
    "\n",
    "SELECT * FROM sample_data_schema_DF WHERE Year > 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_data_schema_DF\n",
    "    .filter(sample_data_schema_DF.Year > 2015)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 DataFrame Transformation Type:  .groupBy()\n",
    "\n",
    "The .groupBy() function will transform a new DF where it performs data aggregation according to the value(s) from a column(s). The SQL syntax would be \"GROUP BY\"\n",
    "\n",
    "Additionally, there is a similar SQL syntax that does the same thing:\n",
    "\n",
    "SELECT RAM, COUNT(*) AS count FROM sample_data_schema_DF GROUP BY RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| RAM|count|\n",
      "+----+-----+\n",
      "|64GB|    1|\n",
      "|16GB|    1|\n",
      "| 8GB|    2|\n",
      "+----+-----+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_data_schema_DF\n",
    "    .groupBy('RAM')\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 DataFrame Transformation Type: .orderBy()\n",
    "\n",
    "The .orderBy() function will transform a new DF where it sorts the results from the specificed columns. The SQL syntac would be \"ORDER BY\"\n",
    "\n",
    "Additionally, there is a similar SQL syntax that does the same thing:\n",
    "\n",
    "SELECT * FROM sample_data_schema_DF ORDER BY W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_data_schema_DF\n",
    "    .orderBy('W')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly, the order of the sorting can be changed from ascending to descending:\n",
    "\n",
    "Additionally, there is a similar SQL syntax that does the same thing:\n",
    "\n",
    "SELECT * FROM sample_data_schema_DF ORDER BY H DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_data_schema_DF\n",
    "    .orderBy(f.col('H').desc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 DataFrame Transformation Type:  .withColumn()\n",
    "\n",
    "The .withColumn() function will transform a new DF which apply the function to a column or literals (with the .lit() method), and stores it as a new function. The SQL syntax would be \"AS\" which assigns a new column name.\n",
    "\n",
    "Additionally, there is a similar SQL syntax that does the same thing:\n",
    "\n",
    "SELECT *, STRING_SPLIT(HDD, ' ') AS HDD_Arrau FROM sample_data_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|    HDDSplit|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|[512GB, SSD]|\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|[256GB, SSD]|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|[128GB, SSD]|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|  [1TB, SSD]|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+"
     ]
    }
   ],
   "source": [
    "# Split the HDD into Size and Type:\n",
    "(\n",
    "    sample_data_schema_DF\n",
    "    .withColumn('HDDSplit', f.split(f.col('HDD'), ' '))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|   HDD_Array|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|[512GB, SSD]|\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|[256GB, SSD]|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|[128GB, SSD]|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|  [1TB, SSD]|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+"
     ]
    }
   ],
   "source": [
    "# Perform the same with .withColumn:\n",
    "(\n",
    "    sample_data_schema_DF\n",
    "    .select(\n",
    "        f.col('*'), f.split(f.col('HDD'), ' ').alias('HDD_Array')\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 DataFrame Transformation Type:  .join()\n",
    "\n",
    "The .join() function allows to join two DataFrames together. Here, the first parameter is the other DF to be joined with. The second paramter specifies which columns to join on. The final parameter defines the nature of the join.\n",
    "\n",
    "Types of Joins are:\n",
    "- inner\n",
    "- outer\n",
    "- full\n",
    "- full_outer\n",
    "- left\n",
    "- left_outer\n",
    "- right\n",
    "- right_outer\n",
    "- left_semi\n",
    "- left_anti\n",
    "\n",
    "The SQL syntax would be the \"JOIN\" statement.\n",
    "\n",
    "Additionally, there is a similar SQL syntax that does the same thing:\n",
    "\n",
    "SELECT a.*, b.FormFactor FROM sample_data_schema AS a LEFT JOIN models_df AS b ON a.Model == b.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|      Model|FormFactor|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|    MacBook|    Laptop|\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|MacBook Pro|    Laptop|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|MacBook Air|    Laptop|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|       iMac|   Desktop|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+"
     ]
    }
   ],
   "source": [
    "# Define the FormFactor:\n",
    "models_df = sc.parallelize([\n",
    "    ('MacBook Pro', 'Laptop'),\n",
    "    ('MacBook', 'Laptop'),\n",
    "    ('MacBook Air', 'Laptop'),\n",
    "    ('iMac', 'Desktop')\n",
    "]).toDF(['Model', 'FormFactor'])\n",
    "\n",
    "# Perform the .join() func.:\n",
    "#\n",
    "(\n",
    "    sample_data_schema_DF\n",
    "    .join(\n",
    "        models_df, sample_data_schema_DF.Model == models_df.Model, 'left'\n",
    "    ).show()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the DataFrame have Models that are not listed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the FormFactor wiht missing data:\n",
    "models_df = sc.parallelize([\n",
    "    ('MacBook Pro', 'Laptop'),\n",
    "    ('MacBook Air', 'Laptop'),\n",
    "    ('iMac', 'Desktop')\n",
    "]).toDF(['Model', 'FormFactor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|      Model|FormFactor|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+\n",
      "|  2|    MacBook|2016|    \"12\\\"\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|       null|      null|\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|MacBook Pro|    Laptop|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|MacBook Air|    Laptop|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|       iMac|   Desktop|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_data_schema_DF\n",
    "    .join(\n",
    "        models_df, sample_data_schema_DF.Model == models_df.Model, 'left'\n",
    "    ).show()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"RIGHT\" join method only keeps the records that are matched with records in the right DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|      Model|FormFactor|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|MacBook Pro|    Laptop|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|MacBook Air|    Laptop|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|       iMac|   Desktop|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+-----------+----------+"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sample_data_schema_DF\n",
    "    .join(\n",
    "        models_df, sample_data_schema_DF.Model == models_df.Model, 'right'\n",
    "    ).show()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other JOIN methods: \"SEMI\" and \"ANTI\"\n",
    "\n",
    "- \"SEMI\" join method will keep all the records that are from the left DF which matches the records from the right DF. BUT it only keeps the columns from the Left DF.\n",
    "\n",
    "- \"ANTI\" join method would be the opposite of the \"SEMI\" join function. It will keep the records that are not found from the rigth DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|    \"15\\\"\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  3|MacBook Air|2016|  \"13.3\\\"\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|    \"27\\\"\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+"
     ]
    }
   ],
   "source": [
    "# \"SEMI\" JOIN:\n",
    "(\n",
    "    sample_data_schema_DF\n",
    "    .join(\n",
    "        models_df, sample_data_schema_DF.Model == models_df.Model, 'left_semi'\n",
    "    ).show()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+---+---------+-----+----+----+------+\n",
      "| Id|  Model|Year|ScreenSize|RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-------+----+----------+---+---------+-----+----+----+------+\n",
      "|  2|MacBook|2016|    \"12\\\"\"|8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "+---+-------+----+----------+---+---------+-----+----+----+------+"
     ]
    }
   ],
   "source": [
    "# \"ANTI\" JOIN:\n",
    "(\n",
    "    sample_data_schema_DF\n",
    "    .join(\n",
    "        models_df, sample_data_schema_DF.Model == models_df.Model, 'left_anti'\n",
    "    ).show()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 DataFrame Transformation Type: .unionAll()\n",
    "\n",
    "The .unionAll() fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.8 DataFrame Transformation Type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.9 DataFrame Transformation Type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.10 DataFrame Transformation Type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.11 DataFrame Transformation Type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.12 DataFrame Transformation Type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11 Overview of the DataFrame Actions:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
