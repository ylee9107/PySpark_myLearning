{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark - Structure Streaming with PySpark\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project aims to explore the workings of Stuructured Streaming with PySpark. As there are an abundance of machine generated data from things like IoT, sensors, devices and beacons. Gaining an insight from these data is becoming more important and requires a quicker response. Streaming such analytics can therefore be a huge differentiator and can be an advantage in business. \n",
    "\n",
    "This project will be combining batch and real time processing to develop continuous applications. The data can be analysed by utilising Spark SQL in batch or in real time, machine learning models will be trained (with MLlib) and followed by scoring these models using Spark Streaming. \n",
    "\n",
    "Apache Spark has been widely adopted due to its ability to unify the disparate data processing paradigms such as Machine learning, SQL and streaming. Companies that uses this are Netflix, Uber, Pinterest etc.\n",
    "\n",
    "The key abstraction in Structure Streaming with PySpark is a discretised stream (DStream) where it represents a stream of data that is divided up tino smaller batches. As these are built on Spark's RDDs, it allows for Spark Streaming to integrate into any other of Spark's components seamlessly such as MLlib or SQL. This unification is one of the key reason of its rapid adoptation in business. It allows developers to use a single framework to perform all processing needs. In short, developers and system administratores can just focus more of their energy on developing smarter solutions/applications.\n",
    "\n",
    "More information:\n",
    "- https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/\n",
    "- https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/\n",
    "- https://databricks.com/session/spark-and-spark-streaming-at-netflix\n",
    "\n",
    "## Running Spark Streaming:\n",
    "\n",
    "Majority of this project will be ran within the System Terminals. This notebook will be used to show the screenshots of the events and its description, like how things are working.\n",
    "\n",
    "## Breakdown of this Notebook\n",
    "\n",
    "- Develope an understanding on DStreams\n",
    "- Develope an understanding on Global Aggregations\n",
    "- Continuous Aggregations with Structured Streaming\n",
    "\n",
    "## 1 PySpark Machine Configuration:\n",
    "\n",
    "Here it only uses four processing cores from the CPU, and it set up by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configures\n",
    "{\n",
    "    \"executorCores\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setup the Correct Directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the Path:\n",
    "path = '++++your working directory here++++/Datasets/'\n",
    "os.chdir(path)\n",
    "folder_pathway = os.getcwd()\n",
    "\n",
    "# print(folder_pathway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 What is Spark Streaming?:\n",
    "\n",
    "To allow for real time processing, Spark's structured streaming is built on DataFrames, this also meaans that it allows for processes like streaming, machine learning and SQL. These are optimised with Spark SQL Engine Catalyst Optimiser (which also receives regular updates). \n",
    "\n",
    "To better understand Spark Streaming, the fundamentals of its predecessor should be explored. \n",
    "\n",
    "### The diagram below shows the data flow of a Spark driver, workers, streaming sources and streaming targets (storage) in a Spark Streaming application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# Import the required library and set to use ggplot:\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "folder_pathway = os.getcwd()\n",
    "image_path = folder_pathway + \"/Description Images/\"\n",
    "\n",
    "# plot the image\n",
    "fig, ax1 = plt.subplots(figsize=(16,10))\n",
    "image = mpimg.imread(image_path + 'Spark Streaming application Data Flow.png')\n",
    "plt.imshow(image);\n",
    "\n",
    "# print('Image source -> ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of what is happening:\n",
    "\n",
    "1. This step starts the Spark Streaming Context (ssc.start() ) where the driver will execute the running taasks on the executors also known as Spark workers.\n",
    "2. With the code already defined within the driver from the Spark Streaming Context, the Receiver on executor 1 will receive a data stream from the Streaming Sources (such as HDFS, Twitter etc., or it is also possible to create a custom receiver). The receiver will also divide up the incoming data stream into blocks and retains it as blocks in the memory.\n",
    "3. As with working with Spark, these data blocks are replicated onto another executor such as executor 2, for the purposes of high availability.\n",
    "4. The block manager in the master node (driver) will have this block ID inforamtion being transmitted onto it. This ensures that each block of the data in the memory is correctly tracked and accounted for.\n",
    "5. The Spark Streaming Context will have every batch interval configured (such as every 1 second) where the driver will launch Spark tasks to process each of these blocks. Lastly, these blocks are persisted to target data stores and these can be cloud storage (S3/WASB), relational data stores (like MySQL, PostgreSQL etc) or No SQL stores.\n",
    "\n",
    "## 4 DStreams:\n",
    "\n",
    "Discretised Streams (DStreams) is the fundamental streaming building block and is built on top of RDDs. It represents a stream of data that is divided into smaller chunks.\n",
    "\n",
    "This section will cover DStreams and performing global aggregations by stateful calculations on it. This is followed by simplifying the streaming application utilising structured streaming and simultaneously gaining performance optimisations. \n",
    "\n",
    "### The diagram below shows that these data chunks are in micro-batches of milliseconds to seconds:\n",
    "\n",
    "This example shows how the lines of DStream is broken down into micro-batches of seconds. Each of the square here represents a micro-batch of each events that has occured within the individual 1 second window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# plot the image\n",
    "fig, ax1 = plt.subplots(figsize=(16, 10))\n",
    "image = mpimg.imread(image_path + 'data chunks in micro batches_1.png')\n",
    "plt.imshow(image);\n",
    "\n",
    "print('Image source -> ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of what is happening:\n",
    "\n",
    "1. First interval, at 1 second: there are 5 occurrences of \"blue\" event and 3 occurrences of \"green\" event.\n",
    "2. Second interval, at the 2nd second: there is a single occurrence of \"gohawks\".\n",
    "3. Fourth interval, at the 4th second: there are 2 occurrences of \"green\" event.\n",
    "\n",
    "## 4.1 Executing in Bash terminal:\n",
    "\n",
    "The above events will be created and executed in a console application (Bash terminal). This section will require two terminals to be opened.\n",
    "\n",
    "Terminal 1: To transmit an event. \\\n",
    "Terminal 2: to receive these events.\n",
    "\n",
    "## 4.1.1 For Terminal 1 -> Netcat Window\n",
    "\n",
    "Use Netcat (nc) to send the events of blue, green and gohawks. To begin, the following commands are used. This will direct the events to port 9999 and that the Spark Streaming job will be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# plot the image\n",
    "fig, ax1 = plt.subplots(figsize=(16, 10))\n",
    "image = mpimg.imread(image_path + 'data chunks in micro batches_2.png')\n",
    "plt.imshow(image);\n",
    "\n",
    "print('Image source -> ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, will be to type in the events taht are desired, where it can be seen in the following diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# plot the image\n",
    "fig, ax1 = plt.subplots(figsize=(16, 10))\n",
    "image = mpimg.imread(image_path + 'data chunks in micro batches_3.png')\n",
    "plt.imshow(image);\n",
    "\n",
    "print('Image source -> ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 For Terminal 2 -> Spark Streaming Window\n",
    "\n",
    "Create a PySpark Streaming application that counts the number of words (events above). The code is constructed in a \" .py \" file and can be executed in the terminal with the following:\n",
    "\n",
    "- Create it as a file in PyCharm or other IDE and call it: streaming_word_count.py\n",
    "- INPUT -> cd /to directory/\n",
    "- INPUT -> ./bin/spark-submit streaming_word_count.py localhost 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# plot the image\n",
    "fig, ax1 = plt.subplots(figsize=(16, 10))\n",
    "image = mpimg.imread(image_path + 'streaming_word_count_pyFile.png')\n",
    "plt.imshow(image);\n",
    "\n",
    "print('Image source -> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Putting it all together:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
