# PySpark - Machine Learning with PySparkâ€™s MLlib

## Introduction

This project will venture into building machine learning models by using the PySpark's MLlib module. It should be noted that it is now being deprecated as it is being moved to the ML module. However, if the datasets used are to be stored on RDDs, it is still possible to utilise the MLlib for machine learning work. 

## Breakdown of this Notebook

- Loading the Dataset
- Exploring the Dataset
- Testing the Dataset
- Transforming the Dataset
- Standardising the Dataset
- Creating RDDs for Training
- Predicting hours of work for census respondents
- Forecasting the income level of census respondents
- Building a clustering model
- Computing the performance statistics


## Dataset:

For this project, the dataset can be found in the "Datasets" folder, where it is sourced from http://archive.ics.uci.edu/ml/datasets/Census+Income. The CSV file that is used is called "". \
Abstract: Predict whether income exceeds $50K/yr based on census data. Also known as "Adult" dataset. 

Alternative source: https://www.kaggle.com/uciml/adult-census-income/data

Included in the "Datasets" folder is two files:
- adults.csv
- census_income_dataset.csv

## Summary:

From this project, I was able to apply the knowledge that I have developed during my project experiences during my days in University but have extended it with further learning with PySpark. This notebook covered the process from loading in the dataset, cleaning and preparing it, testing the data, to modeling and evaluating the performances of the models. I have learnt a lot new ways to implement it the PySpark way whereby dealing with RDDs and using the MLlib to model and predict the data. 
